
# Ready to use LLM-Serving Infrastructure - free, fast, frictionless

## Overview

This project provides a set of scripts and configurations to set up and run a state of the art local LLM-Serving Infrastructure based on ollama and open-webui docker containers - fast, frictionless and with minimal effort. 
Its aimed at lowering the entry barrier to start local LLM experimentation for organsiations of all sizes meeting base level data security standards.
In their current versions (as of May 2024) none of the components share any user data with third parties. All user data stays wihtin the local usage context!
For any open questions including enquiries about commercial use feel free to get in touch!

## Files in this Repository

- /docs contains admin guide and "GettingStarted" docs for endusers 
- container folders contain container config and will store all LLMs, user data and logs for a completely portable setup
- scripts simplify management of the setup

## Prerequisites

To be found in docs/Adminguide.md

## Usage

To be found in docs/Adminguide.md

## Contributing

 - Fork the repository<br>
 - Create a new branch (git checkout -b feature-branch)<br>
 - Make your changes<br>
 - Commit your changes (git commit -am 'Add new feature')<br>
 - Push to the branch (git push origin feature-branch)<br>
 - Create a new Pull Request

## License

This project is licensed under a custom license - see the License.txt file for details.

## Contact

For any inquiries, please contact Marco Thies at thies"at"consultt.net